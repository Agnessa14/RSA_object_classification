{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate the correlations for each layer and create input RDMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import h5py\n",
    "import tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions to create filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFileName(n_samples, name):\n",
    "    return name \\\n",
    "        + \"_{}_\".format(n_samples) \\\n",
    "        + \"_{}_\".format(model_name) \\\n",
    "        + \"_{}\".format(layer_name)  \\\n",
    "        + \".npy\"       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the size of the activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the np file containing the shape of the activations\n",
    "ROOT_PATH = '/mnt/raid/ni/agnessa/RSA/'\n",
    "NR_OF_SAMPLES = 10000\n",
    "json_file_layers=os.path.join(ROOT_PATH,'resnets_selected_layers.json')\n",
    "with open(json_file_layers, \"r\") as fd:\n",
    "    selected_layers = json.load(fd)\n",
    "model_name, layer_name = selected_layers[17].get('model'),selected_layers[17].get('layer') #change the index at each iteration     \n",
    "path = os.path.join(ROOT_PATH + 'activations/', getFileName(NR_OF_SAMPLES, \"shape\"))\n",
    "activations_shape = np.load(path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the correlation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my version\n",
    "def correlationd_matrix(batch_size): #(list_of_activations, n) ,array_activations\n",
    "    total_size,activation_length = activations_shape\n",
    "    print(\"Activation Length: \", activation_length)\n",
    "    print(\"Total Size: \", total_size)\n",
    "\n",
    "    #memmap is used to access a part of the file \n",
    "    file_name = os.path.join(ROOT_PATH+'activations/',getFileName(NR_OF_SAMPLES,'activations'))\n",
    "    act = np.memmap(file_name, mode=\"r\", shape=(total_size, activation_length)) #numsamples x numfeatures\n",
    "    correlationd = np.zeros((total_size,total_size))\n",
    "    correlationd[:] = np.nan\n",
    "    num_batches = int(total_size / batch_size)\n",
    "#     total = sum(x+1 for x in range(num_batches))-num_batches #num 1000-wise comparisons to do: 55-(comparisons of the same sample) = 45\n",
    "    total = sum(x+1 for x in range(num_batches)) #num 1000-wise comparisons to do: 55\n",
    "    index = 0\n",
    "   \n",
    "    for i in range(num_batches):  #num_batches-1 \n",
    "        start_1 = batch_size*i\n",
    "        end_1 = batch_size*(i+1)\n",
    "        list_of_activations_1 = act[start_1:end_1,:]\n",
    "\n",
    "        for j in range(num_batches-i): #(i+1,num_batches): \n",
    "            index += 1\n",
    "            print(\"New Iteration: i = {0}, j = {1}; {2}/{3}\".format(i,j,index,total))\n",
    "            start_2 = batch_size*(i+j)\n",
    "            end_2 = batch_size*(i+j+1)\n",
    "            list_of_activations_2 = act[start_2:end_2,:]\n",
    "            corr_activations = 1-np.corrcoef(list_of_activations_1,list_of_activations_2) #2000 x 2000 matrix\n",
    "            for x in range(corr_activations.shape[0]):\n",
    "                for y in range(corr_activations.shape[1]):\n",
    "                    if x < batch_size:\n",
    "                        start_x = start_1\n",
    "                    else: \n",
    "                        start_x = start_2-1000\n",
    "                    if y < batch_size:\n",
    "                        start_y = start_1\n",
    "                    else:\n",
    "                        start_y = start_2-1000                       \n",
    "                    correlationd[x+start_x,y+start_y] = correlationd[y+start_y,x+start_x] = corr_activations[x,y]\n",
    "        \n",
    "    return(correlationd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Calculating the correlations for model: ',model_name,'and layer: ',layer_name)\n",
    "corr_matrix= correlationd_matrix(1000) \n",
    "path = os.path.join(ROOT_PATH + 'Input_RDM/', getFileName(NR_OF_SAMPLES, \"Input_RDM\"))\n",
    "print(\"Save Input RDM -> {}\".format(path))\n",
    "np.save(path, np.array(corr_matrix)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Felix's version\n",
    "\n",
    "def correlationd_matrix(batch_size): #(list_of_activations, n) ,array_activations\n",
    "    total_size,activation_length = activations_shape\n",
    "    print(\"Activation Length: \", activation_length)\n",
    "    print(\"Total Size: \", total_size)\n",
    "    \n",
    "    def pearsonr_optimized(xm, ss_xm, ym, ss_ym):\n",
    "#         x = np.asarray(x)\n",
    "#         y = np.asarray(y)\n",
    "#         n = len(x)\n",
    "#         mx = x.mean()\n",
    "#         my = y.mean()\n",
    "#         xm, ym = x - mx, y - my\n",
    "        r_num = np.add.reduce(xm * ym)\n",
    "        r_den = np.sqrt(ss_xm * ss_ym)\n",
    "        r = r_num / r_den\n",
    "\n",
    "        # Presumably, if abs(r) > 1, then it is only some small artifact of floating\n",
    "        # point arithmetic.\n",
    "        r = max(min(r, 1.0), -1.0)\n",
    "\n",
    "        return r\n",
    "\n",
    "    #copied directly from scipy sources without change \n",
    "    def ss(a, axis=0):\n",
    "        def _chk_asarray(a, axis):\n",
    "            if axis is None:\n",
    "                a = np.ravel(a)\n",
    "                outaxis = 0\n",
    "            else:\n",
    "                a = np.asarray(a)\n",
    "                outaxis = axis\n",
    "\n",
    "            if a.ndim == 0:\n",
    "                a = np.atleast_1d(a)\n",
    "\n",
    "            return a, outaxis\n",
    "\n",
    "        a, axis = _chk_asarray(a, axis)\n",
    "        return np.sum(a*a, axis)\n",
    "\n",
    "    #memmap is used to access a part of the file. maybe: use memmap in the loop to access the first 1000 samples?    \n",
    "    file_name = os.path.join(ROOT_PATH+'activations/',getFileName(NR_OF_SAMPLES,'activations'))\n",
    "    act = np.memmap(file_name, mode=\"r\", shape=(total_size, activation_length)) #numsamples x numfeatures\n",
    "    correlationd = np.zeros((total_size,total_size))\n",
    "    correlationd[:] = np.nan\n",
    "    total = sum(x+1 for x in range(int(total_size / batch_size))) #num 1000-wise comparisons to do: 55\n",
    "    index = 0\n",
    "    \n",
    "    for i in range(int(total_size / batch_size)):\n",
    "        centered_activations_1 = np.ones((batch_size,activation_length)) * -17\n",
    "        centered_squared_summed_activations_1 = np.ones((batch_size,)) * -17\n",
    "\n",
    "        start_1 = batch_size*i\n",
    "        end_1 = batch_size*(i+1)\n",
    "\n",
    "        list_of_activations_1 = act[start_1:end_1,:]\n",
    "\n",
    "        for j in range(int(total_size / batch_size)-i):\n",
    "            index += 1\n",
    "            print(\"New Iteration: i = {0}, j = {1}; {2}/{3}\".format(i,j,index,total))\n",
    "\n",
    "            start_2 = batch_size*(i+j)\n",
    "            end_2 = batch_size*(i+j+1)\n",
    "\n",
    "            list_of_activations_2 = act[start_2:end_2,:]\n",
    "\n",
    "            centered_activations_2 = np.ones((batch_size,activation_length)) * -17\n",
    "            centered_squared_summed_activations_2 = np.ones((batch_size,)) * -17\n",
    "\n",
    "\n",
    "            for k in range(batch_size):\n",
    "                if k % 200 == 0:\n",
    "                    print(\"Centering... done {0} of {1}\".format(k, batch_size))\n",
    "                centered_activations_1[k] = list_of_activations_1[k] - list_of_activations_1[k].mean()\n",
    "                centered_squared_summed_activations_1[k] = ss(centered_activations_1[k])\n",
    "\n",
    "                centered_activations_2[k] = list_of_activations_2[k] - list_of_activations_2[k].mean()\n",
    "                centered_squared_summed_activations_2[k] = ss(centered_activations_2[k])\n",
    "\n",
    "\n",
    "            for l in range(batch_size):\n",
    "                if l % 200 == 0:\n",
    "                    print(\"Correlation... done {0} of {1}\".format(l, batch_size))\n",
    "                for m in range(batch_size):\n",
    "                    correlationd[start_1+l,start_2+m] = correlationd[start_2+m, start_1+l] = 1 - pearsonr_optimized(centered_activations_1[l], centered_squared_summed_activations_1[l], centered_activations_2[m], centered_squared_summed_activations_2[m])\n",
    "\n",
    "    return(correlationd)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In case the load function does not work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If the load function gives an error, do this\n",
    "np_load_old = np.load # modify the default parameters of np.load\n",
    "np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n",
    "activations_shape = np.load(path)\n",
    "np.load = np_load_old"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
