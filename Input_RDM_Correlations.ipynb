{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate the correlations for each layer and create input RDMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import h5py\n",
    "import tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions to create filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFileName(n_samples, name):\n",
    "    return name \\\n",
    "        + \"_{}_\".format(n_samples) \\\n",
    "        + \"_{}_\".format(model_name) \\\n",
    "        + \"_{}\".format(layer_name)  \\\n",
    "        + \".npy\"\n",
    "#         + datetime.datetime.now().replace(microsecond=0).isoformat() \\\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFileNameh5(n_samples, name):\n",
    "    return name \\\n",
    "        + \"_{}_\".format(n_samples) \\\n",
    "        + \"_{}_\".format(model_name) \\\n",
    "        + \"_{}\".format(layer_name)  \\\n",
    "        + \".h5\"\n",
    "#         + datetime.datetime.now().replace(microsecond=0).isoformat() \\\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the correlation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate INPUT RDM optimized\n",
    "def correlationd_matrix(batch_size): #(list_of_activations, n) ,array_activations\n",
    "    #turn into h5py\n",
    "    #flattened = np.array(activations).reshape(NR_OF_SAMPLES,-1) #if needed\n",
    "#     print(\"Saving into .h5 file\")\n",
    "#     file_name = os.path.join(ROOT_PATH+'activations/',getFileNameh5(NR_OF_SAMPLES,'activations'))\n",
    "#     h5f = h5py.File(file_name, 'w')\n",
    "#     h5f.create_dataset('flattened',dtype='f') #, data=flattened\n",
    "#     h5f.close()\n",
    "#     print(\"Receiving activation length\")\n",
    "\n",
    "    file_name = os.path.join(ROOT_PATH+'activations/',getFileNameh5(NR_OF_SAMPLES,'activations'))\n",
    "#     h5f = h5py.File(file_name, 'r') ?? check if needed\n",
    "    g = tables.open_file(file_name, mode='r') #create tables\n",
    "    total_size, activation_length = np.array(g.root.flattened).shape\n",
    "    print(\"Activation Length: \", activation_length)\n",
    "    print(\"Total Size: \", total_size)\n",
    "    g.close()\n",
    "    \n",
    "#     def pearsonr_optimized(xm, ss_xm, ym, ss_ym):\n",
    "# #         x = np.asarray(x)\n",
    "# #         y = np.asarray(y)\n",
    "# #         n = len(x)\n",
    "# #         mx = x.mean()\n",
    "# #         my = y.mean()\n",
    "# #         xm, ym = x - mx, y - my\n",
    "#         r_num = np.add.reduce(xm * ym)\n",
    "#         r_den = np.sqrt(ss_xm * ss_ym)\n",
    "#         r = r_num / r_den\n",
    "\n",
    "#         # Presumably, if abs(r) > 1, then it is only some small artifact of floating\n",
    "#         # point arithmetic.\n",
    "#         r = max(min(r, 1.0), -1.0)\n",
    "\n",
    "#         return r\n",
    "\n",
    "#     #copied directly from scipy sources without change \n",
    "#     def ss(a, axis=0):\n",
    "#         def _chk_asarray(a, axis):\n",
    "#             if axis is None:\n",
    "#                 a = np.ravel(a)\n",
    "#                 outaxis = 0\n",
    "#             else:\n",
    "#                 a = np.asarray(a)\n",
    "#                 outaxis = axis\n",
    "\n",
    "#             if a.ndim == 0:\n",
    "#                 a = np.atleast_1d(a)\n",
    "\n",
    "#             return a, outaxis\n",
    "\n",
    "#         a, axis = _chk_asarray(a, axis)\n",
    "#         return np.sum(a*a, axis)\n",
    "\n",
    "    #memmap is used to access a part of the file. maybe: use memmap in the loop to access the first 1000 samples?    \n",
    "    act = np.memmap(file_name, mode=\"r\", shape=(total_size, activation_length)) #numsamples x numfeatures\n",
    "    correlationd = np.zeros((total_size,total_size))\n",
    "    correlationd[:] = np.nan\n",
    "    total = sum(x+1 for x in range(int(total_size / batch_size))) #num 1000-wise comparisons to do: 55\n",
    "    index = 0\n",
    "    \n",
    "#     # pre-allocate necessary values for the correlation\n",
    "#     centered_activations = np.ones(list_of_activations.shape) #num samples x num features\n",
    "#     centered_activations[:] = np.nan\n",
    "\n",
    "#     centered_squared_summed_activations = np.ones((list_of_activations.shape[0],)) #num samples x 1\n",
    "#     centered_squared_summed_activations[:] = np.nan\n",
    "    \n",
    "#     #obtain the squared sum of activations\n",
    "#     for i in range(n):      \n",
    "#         centered_activations[i] = list_of_activations[i] - list_of_activations[i].mean()    \n",
    "#         centered_squared_summed_activations[i] = ss(centered_activations[i]) \n",
    "    \n",
    "#     #pre-allocate the input rdm matrix\n",
    "#     correlationd = np.empty((n,n))\n",
    "#     correlationd[:] = np.nan\n",
    "    \n",
    "#     #correlate between each pair of samples\n",
    "#     for i in range(n):\n",
    "#         for j in range(i + 1, n):\n",
    "#             correlationd[i,j] = correlationd[j, i] = 1 - pearsonr_optimized(centered_activations[i], centered_squared_summed_activations[i], centered_activations[j], centered_squared_summed_activations[j])\n",
    "      \n",
    "#     return(correlationd)\n",
    "\n",
    "   \n",
    "    for i in range(int(total_size / batch_size)):\n",
    "       \n",
    "        start_1 = batch_size*i\n",
    "        end_1 = batch_size*(i+1)\n",
    "        \n",
    "        list_of_activations_1 = act[start_1:end_1,:]\n",
    "        \n",
    "        for j in range(int(total_size / batch_size)-i):\n",
    "            index += 1\n",
    "            print(\"New Iteration: i = {0}, j = {1}; {2}/{3}\".format(i,j,index,total))\n",
    "            \n",
    "            start_2 = batch_size*(i+j)\n",
    "            end_2 = batch_size*(i+j+1)\n",
    "            \n",
    "            list_of_activations_2 = act[start_2:end_2,:]\n",
    "            correlationd[start_1:end_1,start_2:end_2] = 1-np.corrcoef(np.concatenate((list_of_activations_1,list_of_activations_2),1))\n",
    "            \n",
    "    return(correlationd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the activations and perform the correlation. Save the results into a matrix (Input RDM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #load json file with the layers of interest (resnets)\n",
    "# print('Loading the json file')\n",
    "# ROOT_PATH = '/mnt/raid/ni/agnessa/RSA/' \n",
    "# json_file_layers=os.path.join(ROOT_PATH,'resnets_selected_layers.json')\n",
    "# with open(json_file_layers, \"r\") as fd:\n",
    "#     selected_layers = json.load(fd)\n",
    "# model_name, layer_name = selected_layers[16].get('model'),selected_layers[16].get('layer') #change the index at each iteration     \n",
    "# NR_OF_SAMPLES = 10000 #num classes*num samples per class;\n",
    "\n",
    "# # #load activations\n",
    "# # path = os.path.join(ROOT_PATH + 'activations/', getFileName(NR_OF_SAMPLES, \"activations\"))\n",
    "\n",
    "# # # save np.load\n",
    "# # np_load_old = np.load\n",
    "\n",
    "# # # modify the default parameters of np.load\n",
    "# # np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n",
    "# # print('Loading activations for model ->',model_name,',layer ->',layer_name)\n",
    "# # flattened = np.load(path)\n",
    "# # np.load = np_load_old\n",
    "\n",
    "# #correlation matrix = input RDM\n",
    "# print('calculating correlations for model ->',model_name, 'and layer ->', layer_name)\n",
    "# corr_matrix = correlationd_matrix(1000) \n",
    "# path = os.path.join(ROOT_PATH + 'Input_RDM/', getFileName(NR_OF_SAMPLES, \"Input_RDM\"))\n",
    "# print(\"Save Input RDM -> {}\".format(path))\n",
    "# np.save(path, np.array(corr_matrix)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "#load json file with the layers of interest (resnets)\n",
    "print('Loading the json file')\n",
    "ROOT_PATH = '/mnt/raid/ni/agnessa/RSA/' \n",
    "json_file_layers=os.path.join(ROOT_PATH,'resnets_selected_layers.json')\n",
    "with open(json_file_layers, \"r\") as fd:\n",
    "    selected_layers = json.load(fd)\n",
    "model_name, layer_name = selected_layers[16].get('model'),selected_layers[16].get('layer') #change the index at each iteration     \n",
    "print('Model ->',model_name,', layer ->', layer_name)\n",
    "NR_OF_SAMPLES = 10000 #num classes*num samples per class;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('calculating correlations for model ->',model_name, 'and layer ->', layer_name)\n",
    "corr_matrix = correlationd_matrix(1000) \n",
    "path = os.path.join(ROOT_PATH + 'Input_RDM/', getFileName(NR_OF_SAMPLES, \"Input_RDM\"))\n",
    "print(\"Save Input RDM -> {}\".format(path))\n",
    "np.save(path, np.array(corr_matrix)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Saving into .h5 file\")\n",
    "file_name = os.path.join(ROOT_PATH+'activations/',getFileNameh5(NR_OF_SAMPLES,'activations'))\n",
    "h5f = h5py.File(file_name, 'w')\n",
    "h5f.create_dataset('flattened',data=flattened) \n",
    "h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create fake h5py file\n",
    "\n",
    "file_name_fake = os.path.join(ROOT_PATH+'activations/',getFileNameh5(NR_OF_SAMPLES,'fake'))\n",
    "hf = h5py.File(file_name_fake, 'w')\n",
    "arr = np.arange(100)\n",
    "hf.create_dataset('fake_dataset',data=arr)\n",
    "# # list(h5f.keys())\n",
    "# fl = hf.get('flattened') \n",
    "# print(flattened.shape)\n",
    "# # flattened = dset.read_direct(fl)\n",
    "# # np.array(fl)\n",
    "# # flattened = np.array(fl)\n",
    "# # # ['flattened'][()]\n",
    "# # type(fl)\n",
    "# # total_size, activation_length = flattened.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open fake dataset\n",
    "file_name_fake = os.path.join(ROOT_PATH+'activations/',getFileNameh5(NR_OF_SAMPLES,'fake'))\n",
    "hfread = h5py.File(file_name_fake, 'r')\n",
    "f = hf.get('fake_dataset') \n",
    "print(f.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load activations\n",
    "path = os.path.join(ROOT_PATH + 'activations/', getFileName(NR_OF_SAMPLES, \"activations\"))\n",
    "\n",
    "# save np.load\n",
    "np_load_old = np.load\n",
    "\n",
    "# modify the default parameters of np.load\n",
    "np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n",
    "print('Loading activations for model ->',model_name,',layer ->',layer_name)\n",
    "flattened = np.load(path)\n",
    "np.load = np_load_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
