{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Set up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets.folder import default_loader\n",
    "import json\n",
    "import torch\n",
    "from torch.autograd import Variable as V\n",
    "import torchvision.models as tmodels\n",
    "import torchvision.transforms as transforms\n",
    "from torch.nn import functional as F\n",
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "os.environ['TORCH_HOME'] = '/mnt/raid/ni/agnessa/RSA/.cache/torch/' #directory where pretrained models are saved\n",
    "imagenet_validation_path = '/mnt/raid/data/ni/dnn/ILSVRC2012_img_val'\n",
    "meta_file_path = '/mnt/raid/ni/agnessa/RSA/'\n",
    "ROOT_PATH = '/mnt/raid/ni/agnessa/RSA/Scenes/ImageNet'\n",
    "layers_path = '/mnt/raid/ni/agnessa/RSA/layer_names'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select data and metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ILSVRCSubDataset(Dataset):\n",
    "    \"\"\"ILSVRC 2012 subset of the original val dataset\"\"\"\n",
    "\n",
    "    def __init__(self, json_file, root, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            json_file (string): Path to the json file with meta.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Parse META File\n",
    "        with open(json_file, \"r\") as fd:\n",
    "            self.meta = json.load(fd)\n",
    "        print(self.meta)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.meta)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = os.path.join(self.root,\n",
    "                            self.meta[idx][\"0\"]) #merge root and the filename of the sample\n",
    "        sample = default_loader(path)\n",
    "        if self.transform is not None:\n",
    "            sample = self.transform(sample)\n",
    "            \n",
    "        wnid = self.meta[idx][\"1\"]\n",
    "            \n",
    "        return sample, wnid #sample, class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the image transformer\n",
    "data_transforms = transforms.Compose([\n",
    "        transforms.Resize((256,256)),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "dataset_val = ILSVRCSubDataset(json_file=os.path.join(meta_file_path,'meta.json'),\n",
    "                               root=imagenet_validation_path,\n",
    "                               transform=data_transforms)\n",
    "\n",
    "dataloaders = torch.utils.data.DataLoader(dataset_val, #Combines a dataset and a sampler, and provides an iterable over the given dataset.\n",
    "                                          batch_size=20, #how many samples per batch to load\n",
    "                                          shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions to create filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFileName(n_samples, name):\n",
    "    return name \\\n",
    "        + \"_{}_\".format(n_samples) \\\n",
    "        + \"_{}_\".format(model_name) \\\n",
    "        + \"_{}\".format(layer_name)  \\\n",
    "        + \".npy\"  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the pretrained model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load json file with the layers of interest \n",
    "model_name = 'densenet161'\n",
    "json_file_layers=os.path.join(layers_path,model_name + '_selected_layers.json')\n",
    "with open(json_file_layers, \"r\") as fd:\n",
    "    selected_layers = json.load(fd)\n",
    "layer_name = selected_layers[15].get('layer') #change the index at each iteration\n",
    "\n",
    "#load the weights\n",
    "model_file = '%s_places365.pth.tar' % model_name\n",
    "model = tmodels.__dict__[model_name](num_classes=365)\n",
    "if not os.access(model_file, os.W_OK):\n",
    "    weight_url = 'http://places2.csail.mit.edu/models_places365/' + model_file\n",
    "    os.system('wget ' + weight_url)\n",
    "    torch.save(model.state_dict(), model_file)\n",
    "    \n",
    "model.load_state_dict(torch.load(model_file))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the activations from a layer for all samples and save them\n",
    "Use the subset with 10 images of 1000 classes on torchvisions pretrained models, get the activations of specific layers and calculate the Input RDM by correlating between the activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NR_OF_SAMPLES = len(dataset_val) #num classes*num samples per class;  len(dataset_val)   \n",
    "batch_size = 20\n",
    "model.eval() #important: put model in evaluation mode for consistent results\n",
    "\n",
    "for layer,m in model.named_modules():\n",
    "    if layer == layer_name:\n",
    "        print('Getting activations for model->',model_name,'and layer->', layer_name)       \n",
    "        data_iterator = iter(dataloaders) \n",
    "        activations = list() \n",
    "        #arguments: model, input, output. every time that an output is computed, this hook is called and the lambda is executed\n",
    "        handle = m.register_forward_hook(lambda m, i, o: activations.append(list(o.data.numpy().squeeze()))) \n",
    "\n",
    "        for i in range(int(NR_OF_SAMPLES/batch_size)): #for each batch get the activations\n",
    "            print(\".\", end='')\n",
    "            cur = next(data_iterator)[0] #cur: images, labels             \n",
    "            out = model(cur) \n",
    "\n",
    "        print('Model->',model_name,'and layer->',layer_name,': done.')\n",
    "            \n",
    "        flattened = np.array(activations).reshape(NR_OF_SAMPLES,-1)\n",
    "        print(\"Shape of the flattened activations -> \",flattened.shape)\n",
    "\n",
    "        #save activations \n",
    "        path = os.path.join(ROOT_PATH + '/activations/', getFileName(NR_OF_SAMPLES,\"activations\"))\n",
    "        print(\"Save Activation -> {}\".format(path))\n",
    "        np.save(path, flattened)\n",
    "\n",
    "        #clear variables\n",
    "        del(activations)\n",
    "        del(data_iterator)\n",
    "        handle.remove() #remove hook   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #if you need to look up the index of a specific model\n",
    "# for idx, dictionary in enumerate(selected_layers):\n",
    "#     if dictionary.get('model') == 'resnet50':\n",
    "#         print(idx)\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##deleted lines of code that might be needed at some point\n",
    "\n",
    "## old code\n",
    "# checkpoint = torch.load(model_file, map_location=lambda storage, loc: storage)\n",
    "# state_dict = {str.replace(k,'module.',''): v for k,v in checkpoint['state_dict'].items()}\n",
    "# model.load_state_dict(state_dict)\n",
    "\n",
    "# updated code\n",
    "# checkpoint = {'state_dict': model.state_dict()}\n",
    "# path_model = os.path.abspath(model_file)\n",
    "# model.load_state_dict(torch.load(path_model)['state_dict'])\n",
    "# model.load_state_dict(torch.load(path_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
