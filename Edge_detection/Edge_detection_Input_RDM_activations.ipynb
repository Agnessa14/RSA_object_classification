{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import argparse\n",
    "import importlib\n",
    "import itertools\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import time\n",
    "from   multiprocessing import Pool\n",
    "import numpy as np\n",
    "import os\n",
    "import pdb\n",
    "import pickle\n",
    "import subprocess\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "# import tensorflow.compat.v1 as tf\n",
    "import tf_slim as slim\n",
    "import threading\n",
    "import scipy.misc\n",
    "from skimage import color\n",
    "from taskonomy.taskbank.lib.models.sample_models import *\n",
    "from taskonomy.taskbank.lib.data.synset import *\n",
    "import scipy\n",
    "import skimage\n",
    "import skimage.io\n",
    "import taskonomy.code.tools.init_paths\n",
    "import transforms3d\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from taskonomy.taskbank.tools.task_viz import *\n",
    "import random\n",
    "import taskonomy.taskbank.tools.utils as utils\n",
    "import models.architectures as architectures\n",
    "from   data.load_ops import resize_rescale_image\n",
    "from   data.load_ops import rescale_image\n",
    "import lib.data.load_ops as load_ops\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenet_validation_path = '/mnt/raid/data/ni/dnn/ILSVRC2012_img_val'\n",
    "meta_file_path = '/mnt/raid/ni/agnessa/RSA/'\n",
    "layers_path = '/mnt/raid/ni/agnessa/RSA/layer_names'\n",
    "ROOT_PATH = '/mnt/raid/ni/agnessa/RSA/Edge_detection/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select Data and get Metadata\n",
    "Select 10 images of each of the 1000 classes of the validation data set together with their label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ILSVRCSubDataset(Dataset):\n",
    "    \"\"\"ILSVRC 2012 subset of the original val dataset\"\"\"\n",
    "\n",
    "    def __init__(self, json_file, root, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            json_file (string): Path to the json file with meta.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Parse META File\n",
    "        with open(json_file, \"r\") as fd:\n",
    "            self.meta = json.load(fd)\n",
    "        print(self.meta)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.meta)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = os.path.join(self.root,\n",
    "                            self.meta[idx][\"0\"]) #merge root and the filename of the sample\n",
    "        sample = default_loader(path)\n",
    "        if self.transform is not None:\n",
    "            sample = self.transform(sample)\n",
    "            \n",
    "        wnid = self.meta[idx][\"1\"]\n",
    "            \n",
    "        return sample, wnid #sample, class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_transforms = transforms.Compose([\n",
    "#     transforms.Resize(255),\n",
    "#     transforms.CenterCrop(224),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "#                          std=[0.229, 0.224, 0.225])\n",
    "# ])\n",
    "\n",
    "dataset_val = ILSVRCSubDataset(json_file=os.path.join(meta_file_path,'meta.json'),\n",
    "                               root=imagenet_validation_path,\n",
    "                               transform=None) #double-check that these transforms are not needed\n",
    "\n",
    "#find a way to iterate in tensorflow\n",
    "# dataloaders = torch.utils.data.DataLoader(dataset_val, #Combines a dataset and a sampler, and provides an iterable over the given dataset.\n",
    "#                                           batch_size=20, #how many samples per batch to load\n",
    "#                                           shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the activations from a layer for all samples and save them\n",
    "Use the subset with 10 images of 1000 classes on torchvisions pretrained models, get the activations of specific layers and calculate the Input RDM by correlating between the activations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions to create filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFileName(n_samples, name):\n",
    "    return name \\\n",
    "        + \"_{}_\".format(n_samples) \\\n",
    "        + \"_{}_\".format(model_name) \\\n",
    "        + \"_{}\".format(layer_name)  \\\n",
    "        + \".npy\"        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load json file with the layers of interest \n",
    "# model_name = 'googlenet'\n",
    "json_file_layers=os.path.join(layers_path,model_name + '_selected_layers.json')\n",
    "with open(json_file_layers, \"r\") as fd:\n",
    "    selected_layers = json.load(fd)\n",
    "layer_name = selected_layers[12].get('layer') #increase the index by one each time you restart the kernel\n",
    "\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "#replace by data iterator\n",
    "# img = '/mnt/raid/data/agnessa/val_256/Places365_val_00015550.jpg/'\n",
    "# store = 'mnt/raid/ni/agnessa/RSA/Scenes/Places365' \n",
    "# task = 'edge2d'\n",
    "\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "def generate_cfg(task):\n",
    "    \n",
    "    repo_dir = '/mnt/raid/ni/agnessa/RSA/Edge_detection/taskonomy/taskbank/'\n",
    "    CONFIG_DIR = os.path.join(repo_dir, 'experiments/final', task)\n",
    "    \n",
    "    ############## Load Configs ##############\n",
    "    import taskonomy.taskbank.tools.utils as utils\n",
    "    import data.load_ops as load_ops\n",
    "    from   general_utils import RuntimeDeterminedEnviromentVars\n",
    "    cfg = utils.load_config( CONFIG_DIR, nopause=True )\n",
    "    RuntimeDeterminedEnviromentVars.register_dict( cfg )\n",
    "    cfg['batch_size'] = 1\n",
    "    if 'batch_size' in cfg['encoder_kwargs']:\n",
    "        cfg['encoder_kwargs']['batch_size'] = 1\n",
    "    cfg['model_path'] = os.path.join( repo_dir, 'temp', task, 'model.permanent-ckpt' )\n",
    "    cfg['root_dir'] = repo_dir\n",
    "    return cfg\n",
    "\n",
    "def run_to_task():\n",
    "    import general_utils\n",
    "    from   general_utils import RuntimeDeterminedEnviromentVars\n",
    "\n",
    "    tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "    \n",
    "    #input info\n",
    "    NR_OF_SAMPLES = 10000 #num classes*num samples per class;  len(dataset_val)   \n",
    "    batch_size = 20\n",
    "    \n",
    "    #replace by data iterator\n",
    "    img = '/mnt/raid/data/agnessa/val_256/Places365_val_00015550.jpg/'\n",
    "    task = 'edge2d'\n",
    "    store = 'mnt/raid/ni/agnessa/RSA/Scenes/Places365' \n",
    "\n",
    "    img = load_raw_image_center_crop( img )\n",
    "    img = Image.fromarray(np.squeeze(img))\n",
    "    cfg = generate_cfg(task)\n",
    "\n",
    "    # Since we observe that areas with pixel values closes to either 0 or 1 sometimes overflows, we clip pixels value\n",
    "    cfg['input_preprocessing_fn'] = load_ops.resize_rescale_image_low_sat\n",
    "    img = cfg[ 'input_preprocessing_fn' ]( img, **cfg['input_preprocessing_fn_kwargs'] )\n",
    "    img = img[np.newaxis,:]\n",
    "    print(\"Doing {task}\".format(task=task))\n",
    "    general_utils = importlib.reload(general_utils)\n",
    "    \n",
    "    from tensorflow.python.framework import ops\n",
    "    ops.reset_default_graph()\n",
    "\n",
    "    training_runners = { 'sess': tf.compat.v1.InteractiveSession(), 'coord': tf.train.Coordinator() }\n",
    "\n",
    "    ############## Set Up Inputs ##############\n",
    "    # tf.logging.set_verbosity( tf.logging.INFO )\n",
    "    setup_input_fn = utils.setup_input\n",
    "    inputs = setup_input_fn( cfg, is_training=False, use_filename_queue=False )\n",
    "    RuntimeDeterminedEnviromentVars.load_dynamic_variables( inputs, cfg )\n",
    "    RuntimeDeterminedEnviromentVars.populate_registered_variables()\n",
    "    start_time = time.time()\n",
    "\n",
    "    ############## Set Up Model ##############\n",
    "    model = utils.setup_model( inputs, cfg, is_training=False )\n",
    "    m = model[ 'model' ]\n",
    "    model[ 'saver_op' ].restore( training_runners[ 'sess' ], cfg[ 'model_path' ] )\n",
    "    \n",
    "    init = tf.compat.v1.global_variables_initializer()\n",
    "    layer_name = 'encoder/block2/unit_1/bottleneck_v1/conv1/Conv2D'\n",
    "    with tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True)) as sess:\n",
    "        sess.run(init)\n",
    "        sess.graph.get_operations()\n",
    "        layer = sess.graph.get_tensor_by_name(f'{layer_name}:0')#,encoder/block2/unit_1/bottleneck_v1/conv1/Conv2D:0')\n",
    "        print('got layer')\n",
    "    \n",
    "        representation = training_runners['sess'].run( \n",
    "            [ layer ], feed_dict={m.input_images: img} )\n",
    "        print('got representation')\n",
    "    \n",
    "\n",
    "#         s_name, file_extension = os.path.splitext(args.store_name)\n",
    "#         with open('{}.npy'.format(s_name), 'wb') as fp:\n",
    "#             np.save(fp, np.squeeze(representation))\n",
    "\n",
    "\n",
    "\n",
    "#         just_rescale = ['autoencoder', 'denoise', 'edge2d', \n",
    "#                     'edge3d', 'keypoint2d', 'keypoint3d',\n",
    "#                     'reshade', 'rgb2sfnorm' ]\n",
    "\n",
    "#         if task in just_rescale:\n",
    "#             simple_rescale_img(predicted, args.store_name)\n",
    "#             return\n",
    "    \n",
    "\n",
    "    ############## Clean Up ##############\n",
    "    training_runners[ 'coord' ].request_stop()\n",
    "    training_runners[ 'coord' ].join()\n",
    "    print(\"Done: {}\".format(config_name))\n",
    "\n",
    "    ############## Reset graph and paths ##############            \n",
    "    tf.reset_default_graph()\n",
    "    training_runners['sess'].close()\n",
    "    return\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run_to_task()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Option 2 - check if this works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cfg(task):\n",
    "    \n",
    "    repo_dir = '/mnt/raid/ni/agnessa/RSA/Edge_detection/taskonomy/taskbank/'\n",
    "    CONFIG_DIR = os.path.join(repo_dir, 'experiments/final', task)\n",
    "    \n",
    "    ############## Load Configs ##############\n",
    "    import taskonomy.taskbank.tools.utils as utils\n",
    "    import data.load_ops as load_ops\n",
    "    from   general_utils import RuntimeDeterminedEnviromentVars\n",
    "    cfg = utils.load_config( CONFIG_DIR, nopause=True )\n",
    "    RuntimeDeterminedEnviromentVars.register_dict( cfg )\n",
    "    cfg['batch_size'] = 1\n",
    "    if 'batch_size' in cfg['encoder_kwargs']:\n",
    "        cfg['encoder_kwargs']['batch_size'] = 1\n",
    "    cfg['model_path'] = os.path.join( repo_dir, 'temp', task, 'model.permanent-ckpt' )\n",
    "    cfg['root_dir'] = repo_dir\n",
    "    return cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load json file with the layers of interest \n",
    "# model_name = 'googlenet'\n",
    "json_file_layers=os.path.join(layers_path,model_name + '_selected_layers.json')\n",
    "with open(json_file_layers, \"r\") as fd:\n",
    "    selected_layers = json.load(fd)\n",
    "layer_name = selected_layers[12].get('layer') #increase the index by one each time you restart the kernel\n",
    "\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "#replace by data iterator\n",
    "# img = '/mnt/raid/data/agnessa/val_256/Places365_val_00015550.jpg/'\n",
    "# store = 'mnt/raid/ni/agnessa/RSA/Scenes/Places365' \n",
    "# task = 'edge2d'\n",
    "\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "\n",
    "import general_utils\n",
    "from   general_utils import RuntimeDeterminedEnviromentVars\n",
    "\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "#input info\n",
    "NR_OF_SAMPLES = 10000 #num classes*num samples per class;  len(dataset_val)   \n",
    "batch_size = 20\n",
    "\n",
    "#replace by data iterator\n",
    "img = '/mnt/raid/data/agnessa/val_256/Places365_val_00015550.jpg/'\n",
    "task = 'edge2d'\n",
    "store = 'mnt/raid/ni/agnessa/RSA/Scenes/Places365' \n",
    "\n",
    "img = load_raw_image_center_crop( img )\n",
    "img = Image.fromarray(np.squeeze(img))\n",
    "cfg = generate_cfg(task)\n",
    "\n",
    "# Since we observe that areas with pixel values closes to either 0 or 1 sometimes overflows, we clip pixels value\n",
    "cfg['input_preprocessing_fn'] = load_ops.resize_rescale_image_low_sat\n",
    "img = cfg[ 'input_preprocessing_fn' ]( img, **cfg['input_preprocessing_fn_kwargs'] )\n",
    "img = img[np.newaxis,:]\n",
    "print(\"Doing {task}\".format(task=task))\n",
    "general_utils = importlib.reload(general_utils)\n",
    "\n",
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()\n",
    "\n",
    "training_runners = { 'sess': tf.compat.v1.InteractiveSession(), 'coord': tf.train.Coordinator() }\n",
    "\n",
    "############## Set Up Inputs ##############\n",
    "# tf.logging.set_verbosity( tf.logging.INFO )\n",
    "setup_input_fn = utils.setup_input\n",
    "inputs = setup_input_fn( cfg, is_training=False, use_filename_queue=False )\n",
    "RuntimeDeterminedEnviromentVars.load_dynamic_variables( inputs, cfg )\n",
    "RuntimeDeterminedEnviromentVars.populate_registered_variables()\n",
    "start_time = time.time()\n",
    "\n",
    "############## Set Up Model ##############\n",
    "model = utils.setup_model( inputs, cfg, is_training=False )\n",
    "m = model[ 'model' ]\n",
    "model[ 'saver_op' ].restore( training_runners[ 'sess' ], cfg[ 'model_path' ] )\n",
    "\n",
    "init = tf.compat.v1.global_variables_initializer()\n",
    "\n",
    "layer_name = 'encoder/block2/unit_1/bottleneck_v1/conv1/Conv2D'\n",
    "with tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True)) as sess:\n",
    "    sess.run(init)\n",
    "    sess.graph.get_operations()\n",
    "    layer = sess.graph.get_tensor_by_name(f'{layer_name}:0')#,encoder/block2/unit_1/bottleneck_v1/conv1/Conv2D:0')\n",
    "    print('got layer')\n",
    "    \n",
    "    #insert the image loop\n",
    "    #put the img transforms here? look in the VGG16 script to see the pipeline\n",
    "    representation = training_runners['sess'].run( \n",
    "        [ layer ], feed_dict={m.input_images: img} )\n",
    "    print('got representation')\n",
    "\n",
    "\n",
    "#         s_name, file_extension = os.path.splitext(args.store_name)\n",
    "#         with open('{}.npy'.format(s_name), 'wb') as fp:\n",
    "#             np.save(fp, np.squeeze(representation))\n",
    "\n",
    "\n",
    "\n",
    "#         just_rescale = ['autoencoder', 'denoise', 'edge2d', \n",
    "#                     'edge3d', 'keypoint2d', 'keypoint3d',\n",
    "#                     'reshade', 'rgb2sfnorm' ]\n",
    "\n",
    "#         if task in just_rescale:\n",
    "#             simple_rescale_img(predicted, args.store_name)\n",
    "#             return\n",
    "\n",
    "\n",
    "############## Clean Up ##############\n",
    "training_runners[ 'coord' ].request_stop()\n",
    "training_runners[ 'coord' ].join()\n",
    "print(\"Done: {}\".format(config_name))\n",
    "\n",
    "############## Reset graph and paths ##############            \n",
    "tf.reset_default_graph()\n",
    "training_runners['sess'].close()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run_to_task()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
