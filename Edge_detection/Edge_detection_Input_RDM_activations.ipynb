{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import argparse\n",
    "import importlib\n",
    "import itertools\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import time\n",
    "from   multiprocessing import Pool\n",
    "import numpy as np\n",
    "import os\n",
    "import pdb\n",
    "import pickle\n",
    "import subprocess\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "# import tensorflow.compat.v1 as tf\n",
    "import tf_slim as slim\n",
    "import threading\n",
    "import scipy.misc\n",
    "from skimage import color\n",
    "from taskonomy.taskbank.lib.models.sample_models import *\n",
    "from taskonomy.taskbank.lib.data.synset import *\n",
    "import scipy\n",
    "import skimage\n",
    "import skimage.io\n",
    "import taskonomy.code.tools.init_paths\n",
    "import transforms3d\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from taskonomy.taskbank.tools.task_viz import *\n",
    "import random\n",
    "import taskonomy.taskbank.tools.utils as utils\n",
    "import models.architectures as architectures\n",
    "from   data.load_ops import resize_rescale_image\n",
    "from   data.load_ops import rescale_image\n",
    "import lib.data.load_ops as load_ops\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "\n",
    "# parser = argparse.ArgumentParser(description='Viz Single Task')\n",
    "\n",
    "# parser.add_argument('--task', dest='task')\n",
    "# parser.set_defaults(task='NONE')\n",
    "\n",
    "# parser.add_argument('--img', dest='im_name')\n",
    "# parser.set_defaults(im_name='NONE')\n",
    "\n",
    "# parser.add_argument('--store', dest='store_name')\n",
    "# parser.set_defaults(store_name='NONE')\n",
    "\n",
    "# parser.add_argument('--store-rep', dest='store_rep', action='store_true')\n",
    "# parser.set_defaults(store_rep=False)\n",
    "\n",
    "# parser.add_argument('--store-pred', dest='store_pred', action='store_true')\n",
    "# parser.set_defaults(store_pred=False)\n",
    "\n",
    "# parser.add_argument('--on-screen', dest='on_screen', action='store_true')\n",
    "# parser.set_defaults(on_screen=False)\n",
    "\n",
    "img = '/mnt/raid/data/agnessa/val_256/Places365_val_00015550.jpg/'\n",
    "task = 'edge2d'\n",
    "store = 'mnt/raid/ni/agnessa/RSA/Scenes/Places365' \n",
    "\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "list_of_tasks = 'autoencoder curvature denoise edge2d edge3d \\\n",
    "keypoint2d keypoint3d colorization jigsaw \\\n",
    "reshade rgb2depth rgb2mist rgb2sfnorm \\\n",
    "room_layout segment25d segment2d vanishing_point \\\n",
    "segmentsemantic class_1000 class_places inpainting_whole'\n",
    "list_of_tasks = list_of_tasks.split()\n",
    "\n",
    "def generate_cfg(task):\n",
    "    \n",
    "#     repo_dir = os.path.dirname(os.path.dirname(os.path.realpath(__file__)))\n",
    "    repo_dir = '/mnt/raid/ni/agnessa/RSA/taskonomy/taskbank/'\n",
    "    CONFIG_DIR = os.path.join(repo_dir, 'experiments/final', task)\n",
    "    ############## Load Configs ##############\n",
    "    import taskonomy.taskbank.tools.utils as utils\n",
    "    import data.load_ops as load_ops\n",
    "    from   general_utils import RuntimeDeterminedEnviromentVars\n",
    "    cfg = utils.load_config( CONFIG_DIR, nopause=True )\n",
    "    RuntimeDeterminedEnviromentVars.register_dict( cfg )\n",
    "    cfg['batch_size'] = 1\n",
    "    if 'batch_size' in cfg['encoder_kwargs']:\n",
    "        cfg['encoder_kwargs']['batch_size'] = 1\n",
    "    cfg['model_path'] = os.path.join( repo_dir, 'temp', task, 'model.permanent-ckpt' )\n",
    "    cfg['root_dir'] = repo_dir\n",
    "    return cfg\n",
    "\n",
    "def run_to_task():\n",
    "    import general_utils\n",
    "    from   general_utils import RuntimeDeterminedEnviromentVars\n",
    "\n",
    "    tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "#     args = parser.parse_args()\n",
    "    \n",
    "    img = '/mnt/raid/data/agnessa/val_256/Places365_val_00015550.jpg/'\n",
    "    task = 'edge2d'\n",
    "    store = 'mnt/raid/ni/agnessa/RSA/Scenes/Places365' \n",
    "\n",
    "    img = load_raw_image_center_crop( img )\n",
    "#     img = skimage.img_as_float(img)\n",
    "    img = Image.fromarray(np.squeeze(img))\n",
    "    \n",
    "#     task = args.task\n",
    "    if task not in list_of_tasks:\n",
    "        raise ValueError('Task not supported')\n",
    "\n",
    "    cfg = generate_cfg(task)\n",
    "\n",
    "    # Since we observe that areas with pixel values closes to either 0 or 1 sometimes overflows, we clip pixels value\n",
    "    low_sat_tasks = 'autoencoder curvature denoise edge2d edge3d \\\n",
    "    keypoint2d keypoint3d \\\n",
    "    reshade rgb2depth rgb2mist rgb2sfnorm \\\n",
    "    segment25d segment2d room_layout'.split()\n",
    "    if task in low_sat_tasks:\n",
    "        cfg['input_preprocessing_fn'] = load_ops.resize_rescale_image_low_sat\n",
    "\n",
    "    if task == 'jigsaw' :\n",
    "        img = cfg[ 'input_preprocessing_fn' ]( img, target=cfg['target_dict'][random.randint(0,99)], \n",
    "                                                **cfg['input_preprocessing_fn_kwargs'] )\n",
    "    else:\n",
    "        img = cfg[ 'input_preprocessing_fn' ]( img, **cfg['input_preprocessing_fn_kwargs'] )\n",
    "\n",
    "    img = img[np.newaxis,:]\n",
    "\n",
    "    if task == 'class_places' or task == 'class_1000':\n",
    "        synset = get_synset(task)\n",
    "\n",
    "    print(\"Doing {task}\".format(task=task))\n",
    "    general_utils = importlib.reload(general_utils)\n",
    "    from tensorflow.python.framework import ops\n",
    "    ops.reset_default_graph()\n",
    "\n",
    "    training_runners = { 'sess': tf.compat.v1.InteractiveSession(), 'coord': tf.train.Coordinator() }\n",
    "\n",
    "    ############## Set Up Inputs ##############\n",
    "    # tf.logging.set_verbosity( tf.logging.INFO )\n",
    "    setup_input_fn = utils.setup_input\n",
    "    inputs = setup_input_fn( cfg, is_training=False, use_filename_queue=False )\n",
    "    RuntimeDeterminedEnviromentVars.load_dynamic_variables( inputs, cfg )\n",
    "    RuntimeDeterminedEnviromentVars.populate_registered_variables()\n",
    "    start_time = time.time()\n",
    "\n",
    "    ############## Set Up Model ##############\n",
    "    model = utils.setup_model( inputs, cfg, is_training=False )\n",
    "    m = model[ 'model' ]\n",
    "    model[ 'saver_op' ].restore( training_runners[ 'sess' ], cfg[ 'model_path' ] )\n",
    "    \n",
    "    init = tf.compat.v1.global_variables_initializer()\n",
    "# # sess = tf.Session() #needed or not?\n",
    "\n",
    "\n",
    "    with tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True)) as sess:\n",
    "        sess.run(init)\n",
    "        sess.graph.get_operations()\n",
    "        layer = sess.graph.get_tensor_by_name('encoder/block2/unit_1/bottleneck_v1/conv1/Conv2D:0')\n",
    "        print('got layer')\n",
    "    \n",
    "    \n",
    "    \n",
    "    #     #layers?\n",
    "# #     name_0 = m.layers[0].name\n",
    "#     import inspect\n",
    "#     attr = inspect.getmembers(model, lambda a:not(inspect.isroutine(a)))\n",
    "#     print(attr)\n",
    "\n",
    "\n",
    "#     init = tf.compat.v1.global_variables_initializer()\n",
    "\n",
    "#     with tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True)) as sess:\n",
    "#         sess.run(init)\n",
    "#         for op in sess.graph.get_operations():\n",
    "#             print(m.layers[0].name)\n",
    "#     # layers? \n",
    "#         for op in sess.graph.get_operations():\n",
    "#             print(op.name)\n",
    "\n",
    "#     for layer in m.layers:\n",
    "#         print(layer.name)\n",
    "    \n",
    "    #this is the important line - need to figure out how to get the representations of diff layers\n",
    "    \n",
    "#     predicted, representation = training_runners['sess'].run( \n",
    "#             [ m.decoder_output,  m.encoder_output ], feed_dict={m.input_images: img} )\n",
    "      \n",
    "    \n",
    "    #try this - potentially works? just need to figure out the name of the layer - check op.name?\n",
    "#     layer = 'encoder_block3_output'    \n",
    "#     layer_rep = getattr(m,layer)\n",
    "#     print(layer_rep)\n",
    "\n",
    "    representation = training_runners['sess'].run( \n",
    "        [ layer ], feed_dict={m.input_images: img} )\n",
    "    print('got representation')\n",
    "    \n",
    "#     if args.store_rep:\n",
    "#         s_name, file_extension = os.path.splitext(args.store_name)\n",
    "#         with open('{}.npy'.format(s_name), 'wb') as fp:\n",
    "#             np.save(fp, np.squeeze(representation))\n",
    "\n",
    "#     if args.store_pred:\n",
    "#         s_name, file_extension = os.path.splitext(args.store_name)\n",
    "#         with open('{}_pred.npy'.format(s_name), 'wb') as fp:\n",
    "#             np.save(fp, np.squeeze(predicted))\n",
    "\n",
    "#     if task == 'segment2d' or task == 'segment25d':\n",
    "#         segmentation_pca(predicted, args.store_name)\n",
    "#         return\n",
    "#     if task == 'colorization':\n",
    "#         single_img_colorize(predicted, img , args.store_name)\n",
    "#         return\n",
    "    \n",
    "#     if task == 'curvature':\n",
    "#         curvature_single_image(predicted, args.store_name)\n",
    "#         return\n",
    "\n",
    "#     just_rescale = ['autoencoder', 'denoise', 'edge2d', \n",
    "#                     'edge3d', 'keypoint2d', 'keypoint3d',\n",
    "#                     'reshade', 'rgb2sfnorm' ]\n",
    "\n",
    "#     if task in just_rescale:\n",
    "#         simple_rescale_img(predicted, args.store_name)\n",
    "#         return\n",
    "    \n",
    "#     just_clip = ['rgb2depth', 'rgb2mist']\n",
    "#     if task in just_clip:\n",
    "#         depth_single_image(predicted, args.store_name)\n",
    "#         return\n",
    "    \n",
    "#     if task == 'inpainting_whole':\n",
    "#         inpainting_bbox(predicted, args.store_name)\n",
    "#         return\n",
    "        \n",
    "#     if task == 'segmentsemantic':\n",
    "#         semseg_single_image( predicted, img, args.store_name)\n",
    "#         return\n",
    "\n",
    "#     if task in ['class_1000', 'class_places']:\n",
    "#         classification(predicted, synset, args.store_name)\n",
    "#         return\n",
    "    \n",
    "#     if task == 'vanishing_point':\n",
    "#         _ = plot_vanishing_point_smoothed(np.squeeze(predicted), (np.squeeze(img) + 1. )/2., args.store_name, [])\n",
    "#         return\n",
    "    \n",
    "#     if task == 'room_layout':\n",
    "#         mean = np.array([0.006072743318127848, 0.010272365569691076, -3.135909774145468, \n",
    "#                         1.5603802322235532, 5.6228218371102496e-05, -1.5669352793761442,\n",
    "#                                     5.622875878174759, 4.082800262277375, 2.7713941642895956])\n",
    "#         std = np.array([0.8669452525283652, 0.687915294956501, 2.080513632043758, \n",
    "#                         0.19627420479282623, 0.014680602791251812, 0.4183827359302299,\n",
    "#                                     3.991778013006544, 2.703495278378409, 1.2269185938626304])\n",
    "#         predicted = predicted * std + mean\n",
    "#         plot_room_layout(np.squeeze(predicted), (np.squeeze(img) + 1. )/2., args.store_name, [], cube_only=True)\n",
    "#         return\n",
    "    \n",
    "#     if task == 'jigsaw':\n",
    "#         predicted = np.argmax(predicted, axis=1)\n",
    "#         perm = cfg[ 'target_dict' ][ predicted[0] ]\n",
    "#         show_jigsaw((np.squeeze(img) + 1. )/2., perm, args.store_name)\n",
    "#         return\n",
    "        \n",
    "#     ############## Clean Up ##############\n",
    "#     training_runners[ 'coord' ].request_stop()\n",
    "#     training_runners[ 'coord' ].join()\n",
    "#     print(\"Done: {}\".format(config_name))\n",
    "\n",
    "#     ############## Reset graph and paths ##############            \n",
    "#     tf.reset_default_graph()\n",
    "#     training_runners['sess'].close()\n",
    "#     return\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run_to_task()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
