{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import argparse\n",
    "import importlib\n",
    "import itertools\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import time\n",
    "from   multiprocessing import Pool\n",
    "import numpy as np\n",
    "import os\n",
    "import pdb\n",
    "import pickle\n",
    "import subprocess\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "# import tensorflow.compat.v1 as tf\n",
    "import tf_slim as slim\n",
    "import threading\n",
    "import scipy.misc\n",
    "from skimage import color\n",
    "from taskonomy.taskbank.lib.models.sample_models import *\n",
    "from taskonomy.taskbank.lib.data.synset import *\n",
    "import scipy\n",
    "import skimage\n",
    "import skimage.io\n",
    "import taskonomy.code.tools.init_paths\n",
    "import transforms3d\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from taskonomy.taskbank.tools.task_viz import *\n",
    "import random\n",
    "import taskonomy.taskbank.tools.utils as utils\n",
    "import models.architectures as architectures\n",
    "from   data.load_ops import resize_rescale_image\n",
    "from   data.load_ops import rescale_image\n",
    "import lib.data.load_ops as load_ops\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "\n",
    "img = '/mnt/raid/data/agnessa/val_256/Places365_val_00015550.jpg/'\n",
    "task = 'edge2d'\n",
    "store = 'mnt/raid/ni/agnessa/RSA/Scenes/Places365' \n",
    "\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "def generate_cfg(task):\n",
    "    \n",
    "    repo_dir = '/mnt/raid/ni/agnessa/RSA/Edge_detection/taskonomy/taskbank/'\n",
    "    CONFIG_DIR = os.path.join(repo_dir, 'experiments/final', task)\n",
    "    \n",
    "    ############## Load Configs ##############\n",
    "    import taskonomy.taskbank.tools.utils as utils\n",
    "    import data.load_ops as load_ops\n",
    "    from   general_utils import RuntimeDeterminedEnviromentVars\n",
    "    cfg = utils.load_config( CONFIG_DIR, nopause=True )\n",
    "    RuntimeDeterminedEnviromentVars.register_dict( cfg )\n",
    "    cfg['batch_size'] = 1\n",
    "    if 'batch_size' in cfg['encoder_kwargs']:\n",
    "        cfg['encoder_kwargs']['batch_size'] = 1\n",
    "    cfg['model_path'] = os.path.join( repo_dir, 'temp', task, 'model.permanent-ckpt' )\n",
    "    cfg['root_dir'] = repo_dir\n",
    "    return cfg\n",
    "\n",
    "def run_to_task():\n",
    "    import general_utils\n",
    "    from   general_utils import RuntimeDeterminedEnviromentVars\n",
    "\n",
    "    tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "    \n",
    "    img = '/mnt/raid/data/agnessa/val_256/Places365_val_00015550.jpg/'\n",
    "    task = 'edge2d'\n",
    "    store = 'mnt/raid/ni/agnessa/RSA/Scenes/Places365' \n",
    "\n",
    "    img = load_raw_image_center_crop( img )\n",
    "    img = Image.fromarray(np.squeeze(img))\n",
    "    cfg = generate_cfg(task)\n",
    "\n",
    "    # Since we observe that areas with pixel values closes to either 0 or 1 sometimes overflows, we clip pixels value\n",
    "    cfg['input_preprocessing_fn'] = load_ops.resize_rescale_image_low_sat\n",
    "    img = cfg[ 'input_preprocessing_fn' ]( img, **cfg['input_preprocessing_fn_kwargs'] )\n",
    "    img = img[np.newaxis,:]\n",
    "    print(\"Doing {task}\".format(task=task))\n",
    "    general_utils = importlib.reload(general_utils)\n",
    "    \n",
    "    from tensorflow.python.framework import ops\n",
    "    ops.reset_default_graph()\n",
    "\n",
    "    training_runners = { 'sess': tf.compat.v1.InteractiveSession(), 'coord': tf.train.Coordinator() }\n",
    "\n",
    "    ############## Set Up Inputs ##############\n",
    "    # tf.logging.set_verbosity( tf.logging.INFO )\n",
    "    setup_input_fn = utils.setup_input\n",
    "    inputs = setup_input_fn( cfg, is_training=False, use_filename_queue=False )\n",
    "    RuntimeDeterminedEnviromentVars.load_dynamic_variables( inputs, cfg )\n",
    "    RuntimeDeterminedEnviromentVars.populate_registered_variables()\n",
    "    start_time = time.time()\n",
    "\n",
    "    ############## Set Up Model ##############\n",
    "    model = utils.setup_model( inputs, cfg, is_training=False )\n",
    "    m = model[ 'model' ]\n",
    "    model[ 'saver_op' ].restore( training_runners[ 'sess' ], cfg[ 'model_path' ] )\n",
    "    \n",
    "    init = tf.compat.v1.global_variables_initializer()\n",
    "\n",
    "    with tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True)) as sess:\n",
    "        sess.run(init)\n",
    "        sess.graph.get_operations()\n",
    "        layer = sess.graph.get_tensor_by_name('encoder/block2/unit_1/bottleneck_v1/conv1/Conv2D:0')\n",
    "        print('got layer')\n",
    "    \n",
    "        representation = training_runners['sess'].run( \n",
    "            [ layer ], feed_dict={m.input_images: img} )\n",
    "        print('got representation')\n",
    "    \n",
    "\n",
    "#         s_name, file_extension = os.path.splitext(args.store_name)\n",
    "#         with open('{}.npy'.format(s_name), 'wb') as fp:\n",
    "#             np.save(fp, np.squeeze(representation))\n",
    "\n",
    "\n",
    "\n",
    "#         just_rescale = ['autoencoder', 'denoise', 'edge2d', \n",
    "#                     'edge3d', 'keypoint2d', 'keypoint3d',\n",
    "#                     'reshade', 'rgb2sfnorm' ]\n",
    "\n",
    "#         if task in just_rescale:\n",
    "#             simple_rescale_img(predicted, args.store_name)\n",
    "#             return\n",
    "    \n",
    "\n",
    "    ############## Clean Up ##############\n",
    "    training_runners[ 'coord' ].request_stop()\n",
    "    training_runners[ 'coord' ].join()\n",
    "    print(\"Done: {}\".format(config_name))\n",
    "\n",
    "    ############## Reset graph and paths ##############            \n",
    "    tf.reset_default_graph()\n",
    "    training_runners['sess'].close()\n",
    "    return\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run_to_task()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
